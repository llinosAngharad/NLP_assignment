<0.6.2.93.16.41.44.garth+@niagara.nectar.cs.cmu.edu (garth gibson).0>
type:     cmu.cs.scs
topic:    wwc: picturetel seminar: patterson, feb 10, 7-<etime>8:30 pm</etime>, 4623 weh
dates:    10-feb-93
time:     <stime>7:00 </stime>- <etime>8:30 pm</etime>
postedby: garth+ on 6-feb-93 at 16:41 from niagara.nectar.cs.cmu.edu (garth gibson)
abstract: 

            west coast colloquium video-conference seminar

            wed feb 10, <location>4623 wean hall</location>, 7:15 pm - 8:15 pm
                moderated locally by garth gibson

	  observations on massively parallel processors and
	     a case for a new theoretical model: logp

		      david a. patterson,
		computer science division/eecs dept.
		 university of california, berkeley

just a few years ago i doubted that massively parallel processors (mpp) 
and software would ever converge on a common foundation, which is 
absolutely essential if mpp is to become popular. today i can see that 
convergence, with a machine operating 1000 times faster than the
fastest cray computer being feasible in just a few years.  now i am 
concerned whether computer science in general (and computer 
science theory in particular) will take advantage of this opportunity
to contribute to and accelerate the success of massive parallelism. 

this talk will first relay my (possibly controversial) observations:
1: we really can get to 1 teraflops(million mflops)! and soon!
2: computer science has an obligation as well as an opportunity in mpp.
3: mpp hardware organizations are converging, and early guesses at mpp 
	issues were wrong.
4: topology based models are not relevant for machines and software of the 90s.
5: current theoretical models (i.e., pram) may be too inaccurate to expect 
	them to lead to important contributions in mpp.

this is followed by an introduction to a more realistic model, called "logp,"
developed by architects and theoreticians at berkeley. the name logp comes
from the four parameters of the model:

 l: latency of communication in the network.
 o: overhead for the processor to send or receive a message from the network.
 g: gap between consecutive messages sent or received at a processor.
 p: number of processor/memory modules.

